Miniforge3-24.11 loaded successful
cuda-12.4 loaded successful
cudnn-8.9.6.50_CUDA 12.x loaded successful
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[rank: 0] Seed set to 42
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/swanlab/integration/pytorch_lightning.py:107: There is a swanlab experiment already in progress and newly created instances of `SwanLabLogger` will reuse this experiment. If this is not desired, call `swanlab.finish()` before instantiating `SwanLabLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name | Type           | Params | Mode 
------------------------------------------------
0 | net  | PolicyValueNet | 268 K  | train
------------------------------------------------
268 K     Trainable params
0         Non-trainable params
268 K     Total params
1.075     Total estimated model params size (MB)
26        Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('counters/total_env_steps', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'counters/total_env_steps': ...})` instead.
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('rollout/T', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'rollout/T': ...})` instead.
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('rollout/B', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'rollout/B': ...})` instead.
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('rollout/samples', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'rollout/samples': ...})` instead.
/data/home/sczc457/.conda/envs/rule_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('stats/episodes', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'stats/episodes': ...})` instead.
`Trainer.fit` stopped: `max_steps=10000` reached.
